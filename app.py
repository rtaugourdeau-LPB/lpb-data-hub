import re
import time
import csv
import zipfile
import json
from io import BytesIO
from typing import List, Tuple, Optional
from datetime import datetime
import math

import requests
import psycopg2
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from sqlalchemy import create_engine, text
from rapidfuzz import process, fuzz
from unidecode import unidecode

# ============================================================
# âš™ï¸ CONFIG GENERALE STREAMLIT (UNE SEULE FOIS)
# ============================================================

st.set_page_config(
    page_title="Outils Data LPB",
    page_icon="ðŸ§±",
    layout="wide",
)

# Date du jour pour les exports
TODAY_STR = datetime.now().strftime("%Y-%m-%d")
MAX_ROWS_EXPORT = 1_000_000      # limite globale (pour CSV/ZIP et Excel)
ROWS_PER_CSV_FILE = 200_000      # lignes max par fichier CSV dans le ZIP
MAX_ROWS_EXCEL = 200_000         # limite stricte pour Excel (Ã©vite les xlsx Ã©normes)

# ============================================================
# ðŸ” CONFIG VIA st.secrets (Streamlit)
# ============================================================

# --- Airtable (Votes) ---
AIRTABLE_TOKEN   = st.secrets["AIRTABLE_TOKEN"]
AIRTABLE_BASE_ID = st.secrets["AIRTABLE_BASE_ID"]
AIR_H = {"Authorization": f"Bearer {AIRTABLE_TOKEN}"}

# --- Notion (Data Hub) ---
NOTION_TOKEN     = st.secrets["NOTION_TOKEN"]
DB_BAC_SABLE     = st.secrets["DB_BAC_SABLE"]
DB_NOTION_PROJET = st.secrets["DB_NOTION_PROJET"]

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Notion-Version": "2022-06-28",
    "Content-Type": "application/json",
}

# --- PostgreSQL (BO / Data Hub) ---
PG_HOST     = st.secrets["PG_HOST"]
PG_PORT     = st.secrets["PG_PORT"]
PG_DB       = st.secrets["PG_DB"]
PG_USER     = st.secrets["PG_USER"]
PG_PASSWORD = st.secrets["PG_PASSWORD"]
PG_SSLMODE  = st.secrets.get("PG_SSLMODE", "require")

# URL publique projet LPB (utilisÃ©e dans page_votes)
LPB_PROJECT_URL = st.secrets.get(
    "LPB_PROJECT_URL",
    "https://app.lapremierebrique.fr/fr/projects/{project_id}",
)

# variables utilisÃ©es par psycopg2
PGHOST     = PG_HOST
PGPORT     = PG_PORT
PGDATABASE = PG_DB
PGUSER     = PG_USER
PGPASSWORD = PG_PASSWORD
PGSSLMODE  = PG_SSLMODE

# =============================================================================
# ðŸ§  Ã‰TAT GLOBAL STREAMLIT (pour Data Hub)
# =============================================================================

# API Notion
if "api_df" not in st.session_state:
    st.session_state.api_df = None

# Import ZIP/CSV
if "imp_df" not in st.session_state:
    st.session_state.imp_df = None
if "imp_filename" not in st.session_state:
    st.session_state.imp_filename = None

# Back-office (PostgreSQL)
if "bo_df" not in st.session_state:
    st.session_state.bo_df = None
if "bo_table_id" not in st.session_state:
    st.session_state.bo_table_id = None  # full_name = schema.table
if "bo_tables_df" not in st.session_state:
    st.session_state.bo_tables_df = None


# =============================================================================
# ðŸ”§ FONCTIONS COMMUNES (utilisÃ©es par les 2 modules)
# =============================================================================

def ensure_ok(r: requests.Response):
    """Raise si HTTP != 2xx, avec dÃ©tail."""
    try:
        r.raise_for_status()
    except Exception as e:
        try:
            detail = r.json()
        except Exception:
            detail = r.text
        raise RuntimeError(f"HTTP {r.status_code}: {detail}") from e


# ======================= Utils strings (Votes) =======================

def normalize_name(s: str) -> str:
    """Normalisation douce pour comparer les noms."""
    if s is None:
        return ""
    s2 = unidecode(str(s)).lower().strip()
    s2 = re.sub(r"\s+", " ", s2)
    s2 = re.sub(r"^(le|la|les|l')\s+", "", s2)
    return s2


def normalize_email(s: str) -> str:
    if s is None:
        return ""
    return unidecode(str(s)).strip().lower()


def extract_dept(token: str) -> Optional[int]:
    """Extrait (DD) ou (DDD) en fin de libellÃ© â†’ int, sinon None."""
    m = re.search(r"\((\d{2,3})\)\s*$", str(token).strip())
    return int(m.group(1)) if m else None


def make_nom_airtable(name, department) -> str:
    """Construit 'Nom (DD)' Ã  partir de BO (name, department)."""
    n = "" if pd.isna(name) else str(name).strip()
    if pd.isna(department) or str(department).strip() == "":
        return n
    try:
        dep = int(department)
    except Exception:
        dep = str(department).strip()
    return f"{n} ({dep})"


# ======================= Patterns globaux prolongation / pouvoir =======================

PROLONG_PATTERN = re.compile(
    r"(prolongation|pr[Ã©e]pa|pr[Ã©e]par|pr[Ã©e]paration|prepa|ne\s*se\s*pron|nspp)",
    re.I,
)
POUVOIR_PATTERN = re.compile(
    r"(pouvoir|procuration|proxy)",
    re.I,
)

# =============================================================================
# ðŸ”§ FONCTIONS AIRTABLE (Votes)
# =============================================================================

def list_tables_with_views(base_id: str) -> List[dict]:
    """Retourne la mÃ©tadonnÃ©e Airtable de toutes les tables de la base."""
    if not AIR_H:
        return []
    r = requests.get(
        f"https://api.airtable.com/v0/meta/bases/{base_id}/tables",
        headers=AIR_H,
        timeout=30,
    )
    ensure_ok(r)
    return r.json().get("tables", []) or []


def table_has_prolongation_or_pouvoir(table_meta: dict) -> bool:
    """
    Ne garde que les tables qui ont AU MOINS une colonne liÃ©e Ã  'prolongation' OU 'pouvoir'.
    """
    for f in table_meta.get("fields", []) or []:
        name = str(f.get("name", ""))
        if PROLONG_PATTERN.search(name) or POUVOIR_PATTERN.search(name):
            return True
    return False


def fetch_view_records(
    base_id: str,
    table_id_or_name: str,
    view_id_or_name: str,
    page_size: int = 100,
) -> List[dict]:
    if not AIR_H:
        return []
    url = f"https://api.airtable.com/v0/{base_id}/{table_id_or_name}"
    params = {"pageSize": page_size, "view": view_id_or_name}
    out, offset = [], None
    while True:
        if offset:
            params["offset"] = offset
        r = requests.get(url, headers=AIR_H, params=params, timeout=60)
        ensure_ok(r)
        data = r.json()
        out.extend(data.get("records", []))
        offset = data.get("offset")
        if not offset:
            break
        time.sleep(0.12)  # respect soft-rate
    return out


def flatten(records: List[dict]) -> pd.DataFrame:
    if not records:
        return pd.DataFrame()
    rows = []
    for r in records:
        d = {"_air_id": r.get("id"), "_air_createdTime": r.get("createdTime")}
        d.update(r.get("fields", {}) or {})
        rows.append(d)
    return pd.DataFrame(rows)


# =============================================================================
# ðŸ”§ FONCTIONS BO (Votes) â€” SQLAlchemy
# =============================================================================

@st.cache_resource(show_spinner=False)
def get_engine():
    """Engine SQLAlchemy sur le BO (read-replica)."""
    if not all([PG_HOST, PG_DB, PG_USER, PG_PASSWORD]):
        return None
    uri = (
        f"postgresql+psycopg2://{PG_USER}:{PG_PASSWORD}"
        f"@{PG_HOST}:{PG_PORT}/{PG_DB}?sslmode={PG_SSLMODE}"
    )
    return create_engine(uri, pool_pre_ping=True)


@st.cache_data(show_spinner=True)
def load_projects_df() -> pd.DataFrame:
    eng = get_engine()
    if eng is None:
        return pd.DataFrame(
            columns=["id", "name", "department", "nom_airtable", "name_norm", "nom_airtable_norm"]
        )
    q = text(
        """
        SELECT id, name, department
        FROM public.projects
        WHERE name IS NOT NULL
        """
    )
    df = pd.read_sql(q, eng)
    df["nom_airtable"] = df.apply(
        lambda r: make_nom_airtable(r["name"], r["department"]),
        axis=1,
    )
    df["name_norm"] = df["name"].apply(normalize_name)
    df["nom_airtable_norm"] = df["nom_airtable"].apply(normalize_name)
    return df


@st.cache_data(show_spinner=True)
def load_subs_for_project(project_id: int) -> pd.DataFrame:
    eng = get_engine()
    if eng is None:
        return pd.DataFrame(
            columns=[
                "subscription_id",
                "users_profile_id",
                "project_id",
                "subscribed_at",
                "email_normalized",
                "email_raw",
            ]
        )
    q = text(
        """
        SELECT
            s.id AS subscription_id,
            s.users_profile_id AS users_profile_id,
            s.project_id,
            s.created_at AS subscribed_at,
            lower(trim(u.email)) AS email_normalized,
            u.email AS email_raw
        FROM public.subscriptions s
        JOIN public.users_profiles up ON up.id = s.users_profile_id
        LEFT JOIN public.users u ON u.id = up.user_id
        WHERE s.status <> 'canceled'
          AND s.project_id = :pid
          AND u.email IS NOT NULL
        """
    )
    return pd.read_sql(q, eng, params={"pid": int(project_id)})

@st.cache_data(show_spinner=True)
def load_invest_amounts_for_project(project_id: int) -> pd.DataFrame:
    """
    Montant investi total par email sur ce projet (pondÃ©ration).
    IMPORTANT : adapte le champ montant si besoin (amount, amount_cents, etc.)
    """
    eng = get_engine()
    if eng is None:
        return pd.DataFrame(columns=["email_normalized", "invest_amount_eur"])

    q = text(
        """
        SELECT
            lower(trim(u.email)) AS email_normalized,
            SUM(s.amount) AS invest_amount_eur
        FROM public.subscriptions s
        JOIN public.users_profiles up ON up.id = s.users_profile_id
        LEFT JOIN public.users u ON u.id = up.user_id
        WHERE s.status <> 'canceled'
          AND s.project_id = :pid
          AND u.email IS NOT NULL
        GROUP BY 1
        """
    )
    return pd.read_sql(q, eng, params={"pid": int(project_id)})
    
# =============================================================================
# ðŸ”§ Matching Projet (Airtable â†’ BO)
# =============================================================================

def guess_candidates_from_label(
    label: str,
    projects_df: pd.DataFrame,
    topn: int = 10,
) -> List[Tuple[int, str, float]]:
    cand: List[Tuple[int, str, float]] = []
    label = (label or "").strip()
    dept = extract_dept(label)
    label_no_dep = re.sub(r"\s*\(\d{2,3}\)\s*$", "", label).strip()

    # 1) exact name + dept
    if dept is not None:
        m1 = projects_df.query("department == @dept and name == @label_no_dep")
        for _, r in m1.iterrows():
            cand.append((int(r["id"]), r["nom_airtable"], 100.0))

    # 2) exact name seul
    m2 = projects_df.query("name == @label_no_dep")
    for _, r in m2.iterrows():
        tup = (int(r["id"]), r["nom_airtable"], 99.0)
        if tup not in cand:
            cand.append(tup)

    # 3) exact normalisÃ©
    label_norm = normalize_name(label_no_dep)
    m3 = projects_df[projects_df["name_norm"] == label_norm]
    for _, r in m3.iterrows():
        tup = (int(r["id"]), r["nom_airtable"], 98.0)
        if tup not in cand:
            cand.append(tup)

    # 4) fuzzy sur nom_airtable puis name
    choices = projects_df["nom_airtable"].tolist()
    for (disp, score, pos) in process.extract(
        label,
        choices,
        scorer=fuzz.token_set_ratio,
        limit=topn,
    ):
        r = projects_df.iloc[pos]
        tup = (int(r["id"]), r["nom_airtable"], float(score))
        if tup not in cand:
            cand.append(tup)

    choices2 = projects_df["name"].tolist()
    for (disp, score, pos) in process.extract(
        label_no_dep,
        choices2,
        scorer=fuzz.token_set_ratio,
        limit=topn,
    ):
        r = projects_df.iloc[pos]
        tup = (int(r["id"]), r["nom_airtable"], float(score) - 0.5)
        if tup not in cand:
            cand.append(tup)

    return sorted(cand, key=lambda x: x[2], reverse=True)[:topn]


def pick_project_id_from_airtable(
    df_view: pd.DataFrame,
    projects_df: pd.DataFrame,
    view_name: str,
    table_name: Optional[str] = None,
) -> Tuple[int, str]:
    """
    Ordre de rÃ©solution :
    1) colonne 'nom de projet' dans la vue (Nom du projet, Projet, Project nameâ€¦)
    2) colonne URL â†’ extrait /projects/<id>
    3) fallback depuis nom de vue ou de table
    """

    candidate_label = None

    # (1) colonne â€˜nom projetâ€™
    name_cols = [
        c
        for c in df_view.columns
        if re.search(r"(nom.*projet|projet.*nom|project.?name)", str(c), re.I)
    ]
    if name_cols:
        s = df_view[name_cols[0]].dropna().astype(str).str.strip()
        if not s.empty:
            candidate_label = s.value_counts().idxmax()

    # (2) colonne lien â†’ ID
    if candidate_label is None:
        url_cols = [
            c for c in df_view.columns if re.search(r"(url|lien|link)", str(c), re.I)
        ]
        if url_cols:
            s = df_view[url_cols[0]].dropna().astype(str)
            m = s.str.extract(r"/projects/(\d+)", expand=False).dropna()
            if not m.empty:
                pid = int(m.iloc[0])
                disp = projects_df.loc[projects_df["id"] == pid, "nom_airtable"]
                if not disp.empty:
                    return pid, disp.iloc[0]

    # (3) fallbacks intelligents
    def extract_from_title(txt: str) -> str:
        if not txt:
            return ""
        m = re.search(r"Projet\s+(.+)$", txt, flags=re.I)
        return m.group(1).strip() if m else txt.strip()

    if candidate_label is None:
        looks_like_view_id = bool(re.match(r"^Vue\s+viw[A-Za-z0-9]+$", str(view_name)))
        if table_name and (looks_like_view_id or "Projet" in str(table_name)):
            candidate_label = extract_from_title(str(table_name))
        else:
            candidate_label = extract_from_title(str(view_name))

    st.write("LibellÃ© de croisement Airtable :", f"{candidate_label}")

    cands = guess_candidates_from_label(candidate_label, projects_df, topn=5)

    # DÃ©dupliquer par project_id en gardant le meilleur score
    best = {}
    for pid, disp, score in cands:
        if (pid not in best) or (score > best[pid][1]):
            best[pid] = (disp, float(score))
    cands_unique = [(pid, disp, score) for pid, (disp, score) in best.items()]

    # âš–ï¸ Tri : d'abord les projets NON "Canceled", puis score dÃ©croissant, puis libellÃ©
    def is_canceled(name: str) -> int:
        # 0 = projet normal, 1 = projet "Canceled"
        return 1 if re.search(r"\bcanceled\b", str(name), flags=re.I) else 0

    cands_unique.sort(
        key=lambda x: (
            is_canceled(x[1]),  # non-canceled (0) avant canceled (1)
            -x[2],              # score dÃ©croissant
            x[1],               # puis libellÃ©
        )
    )

    options = [
        f"{pid} â€” {disp} (score {score:.1f})"
        for pid, disp, score in cands_unique
    ] or ["Saisir manuellement"]

    choice = st.selectbox(
        "Confirme le projet exact BO (les plus probables) :",
        options,
        index=0,
    )

    if choice == "Saisir manuellement":
        pid = st.number_input("ID projet LPB :", min_value=1, step=1)
        if pid in projects_df["id"].values:
            disp = projects_df.loc[projects_df["id"] == pid, "nom_airtable"].iloc[0]
        else:
            disp = f"Projet {int(pid)}"
        return int(pid), disp

    pid = int(re.match(r"^(\d+)\sâ€”", choice).group(1))
    disp = projects_df.loc[projects_df["id"] == pid, "nom_airtable"].iloc[0]
    return pid, disp


# =============================================================================
# ðŸ”§ DÃ©tection colonnes (emails / prolongation / pouvoir) â€” Votes
# =============================================================================

def detect_email_columns(df: pd.DataFrame) -> List[str]:
    cols = [c for c in df.columns if re.search(r"mail|e-?mail", str(c), re.I)]
    if cols:
        return cols
    # fallback: premiÃ¨re colonne contenant '@'
    for c in df.columns:
        try:
            if df[c].astype(str).str.contains("@").any():
                return [c]
        except Exception:
            pass
    return []


def detect_prolongation_column(df: pd.DataFrame) -> Optional[str]:
    for c in df.columns:
        if PROLONG_PATTERN.search(str(c)):
            return c
    return None


def detect_pouvoir_column(df: pd.DataFrame) -> Optional[str]:
    for c in df.columns:
        if POUVOIR_PATTERN.search(str(c)):
            return c
    return None


def standardize_prolongation(val) -> str:
    """
    Prolongation = rÃ©ponse Ã  :
    - si colonne 'pouvoir' prÃ©sente :
        Â« ÃŠtes-vous d'accord pour accorder la prolongation ? Â»
    - sinon :
        Â« ÃŠtes-vous d'accord pour appliquer la prolongation, avec application des 5% de pÃ©nalitÃ© ? Â»
    """
    if pd.isna(val):
        return "Non renseignÃ©"
    s = unidecode(str(val)).strip().lower()
    if s in {"oui", "o", "yes", "y", "true", "1"}:
        return "Oui"
    if s in {"non", "n", "no", "false", "0"}:
        return "Non"
    if re.search(r"ne\s*se\s*pron", s) or s in {"nspp", "ne se prononce pas"}:
        return "Ne se prononce pas"
    return str(val)


def standardize_pouvoir(val) -> str:
    """
    Pouvoir = rÃ©ponse Ã  : Â« ÃŠtes-vous d'accord pour NE PAS APPLIQUER les pÃ©nalitÃ©s ? Â»
    """
    if pd.isna(val):
        return "Non renseignÃ©"
    s = unidecode(str(val)).strip().lower()
    if s in {"oui", "o", "yes", "y", "true", "1"}:
        return "Oui"
    if s in {"non", "n", "no", "false", "0"}:
        return "Non"
    if re.search(r"ne\s*se\s*pron", s) or s in {"nspp", "ne se prononce pas"}:
        return "Ne se prononce pas"
    if s in {"", "nan"}:
        return "Non renseignÃ©"
    return str(val)


def build_votes_email_flags(
    df_view: pd.DataFrame,
    email_cols: List[str],
    prolong_col: Optional[str],
    pouvoir_col: Optional[str],
) -> pd.DataFrame:
    """
    AgrÃ©gÃ© par e-mail :
    - email_normalized
    - email_raw_example
    - prolongation (mode standardisÃ©)
    - pouvoir (mode standardisÃ©)
    - n_occur
    """
    if not email_cols:
        return pd.DataFrame(
            columns=["email_normalized", "email_raw_example", "prolongation", "pouvoir"]
        )

    melted = []
    for c in email_cols:
        if c not in df_view.columns:
            continue
        s = df_view[[c]].copy()
        s = s.dropna()
        if s.empty:
            continue
        s["email_raw"] = s[c].astype(str)
        s["email_normalized"] = s["email_raw"].map(normalize_email)
        s["source_col"] = c

        if prolong_col and prolong_col in df_view.columns:
            s["prolongation_raw"] = df_view.loc[s.index, prolong_col]
        else:
            s["prolongation_raw"] = None

        if pouvoir_col and pouvoir_col in df_view.columns:
            s["pouvoir_raw"] = df_view.loc[s.index, pouvoir_col]
        else:
            s["pouvoir_raw"] = None

        melted.append(
            s[
                [
                    "email_raw",
                    "email_normalized",
                    "source_col",
                    "prolongation_raw",
                    "pouvoir_raw",
                ]
            ]
        )

    if not melted:
        return pd.DataFrame(
            columns=["email_normalized", "email_raw_example", "prolongation", "pouvoir"]
        )

    tmp = pd.concat(melted, ignore_index=True)
    tmp = tmp[tmp["email_normalized"].str.contains("@", na=False)]

    tmp["prolongation_std"] = tmp["prolongation_raw"].apply(standardize_prolongation)
    tmp["pouvoir_std"] = tmp["pouvoir_raw"].apply(standardize_pouvoir)

    agg = (
        tmp.groupby("email_normalized", as_index=False)
        .agg(
            n_occur=("email_normalized", "size"),
            email_raw_example=("email_raw", "first"),
            prolongation=(
                "prolongation_std",
                lambda x: x.dropna().mode().iloc[0]
                if not x.dropna().empty
                else "Non renseignÃ©",
            ),
            pouvoir=(
                "pouvoir_std",
                lambda x: x.dropna().mode().iloc[0]
                if not x.dropna().empty
                else "Non renseignÃ©",
            ),
        )
        .sort_values("email_normalized")
    )
    return agg


# =============================================================================
# ðŸ”§ FONCTIONS EXPORT (Data Hub)
# =============================================================================

def df_to_excel_bytes(df: pd.DataFrame, sheet_name: str = "Data"):
    """
    Export propre vers Excel :
    - Table structurÃ©e avec style
    - Largeur de colonnes auto
    - Ligne dâ€™en-tÃªte figÃ©e
    - Filtres auto
    - Hyperliens dÃ©sactivÃ©s
    """
    out = BytesIO()
    df_export = df.copy()

    # Colonnes plutÃ´t texte
    text_col_keywords = (
        "url",
        "link",
        "href",
        "file",
        "image",
        "img",
        "path",
        "uuid",
        "token",
        "hash",
        "id",
        "code",
        "ref",
        "reference",
        "email",
        "mail",
        "phone",
        "tel",
    )

    for col in df_export.columns:
        s = df_export[col]
        if not (pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s)):
            continue

        col_lower = str(col).lower()
        force_text = any(k in col_lower for k in text_col_keywords)

        s_str = s.astype(str)

        url_like = s_str.str.startswith(("http://", "https://", "www."))
        email_like = s_str.str.contains("@", na=False)
        long_like = s_str.str.len() > 120

        if url_like.mean() > 0.1 or email_like.mean() > 0.3 or long_like.mean() > 0.3:
            force_text = True

        def to_excel_safe(v):
            # â›”ï¸ ne pas utiliser pd.isna ici (problÃ¨me avec les arrays)
            if v is None or (isinstance(v, float) and math.isnan(v)):
                return None
            sv = str(v)
            if sv.startswith(("=", "+", "-", "@")):
                sv = "'" + sv
            if sv.startswith(("http://", "https://", "www.")):
                sv = "'" + sv
            return sv

        if force_text:
            df_export[col] = s_str.map(to_excel_safe)
        else:
            def sanitize(v):
                # â›”ï¸ idem : uniquement scalaires
                if v is None or (isinstance(v, float) and math.isnan(v)):
                    return None
                sv = v
                if isinstance(sv, str):
                    if sv.startswith(("=", "+", "-", "@")):
                        sv = "'" + sv
                    if sv.startswith(("http://", "https://", "www.")) and len(sv) > 255:
                        sv = "'" + sv
                return sv

            df_export[col] = s.map(sanitize)

    with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
        df_export.to_excel(writer, index=False, sheet_name=sheet_name)
        workbook = writer.book
        worksheet = writer.sheets[sheet_name]
        n_rows, n_cols = df_export.shape

        # Largeur de colonnes auto
        for col_idx, col_name in enumerate(df_export.columns):
            col_series = df_export[col_name].astype(str).fillna("")
            sample = col_series.head(500)
            max_len_value = sample.map(len).max() if not sample.empty else 0
            max_len = max(len(str(col_name)), max_len_value)
            base_width = min(max_len + 2, 60)
            width = base_width + 4
            worksheet.set_column(col_idx, col_idx, width)

        worksheet.freeze_panes(1, 0)

        if n_cols > 0:
            table_columns = [{"header": str(col)} for col in df_export.columns]
            worksheet.add_table(
                0,
                0,
                n_rows,
                n_cols - 1,
                {
                    "columns": table_columns,
                    "style": "Table Style Medium 2",
                    "autofilter": True,
                },
            )

        worksheet.set_zoom(100)

    return out.getvalue()


def df_to_csv_bytes(df: pd.DataFrame):
    """Export CSV simple, compatible Excel FR."""
    csv_str = df.to_csv(index=False, sep=";", encoding="utf-8-sig")
    return csv_str.encode("utf-8-sig")

def chunk_df_to_zip_csv_bytes(df: pd.DataFrame, rows_per_file: int, base_name: str) -> bytes:
    """DÃ©coupe un DF en plusieurs CSV et renvoie un ZIP (bytes)."""
    out = BytesIO()
    with zipfile.ZipFile(out, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        n = len(df)
        n_files = max(1, math.ceil(n / rows_per_file))
        for i in range(n_files):
            start = i * rows_per_file
            end = min((i + 1) * rows_per_file, n)
            chunk = df.iloc[start:end]
            csv_bytes = df_to_csv_bytes(chunk)
            zf.writestr(f"{base_name}_{i+1:03d}.csv", csv_bytes)
    return out.getvalue()

# =============================================================================
# ðŸ”§ FONCTIONS NOTION (Data Hub)
# =============================================================================
######################################################################################
@st.cache_data(show_spinner=False)
def load_notion_projects_df() -> pd.DataFrame:
    """
    Charge la base Notion des projets (DB_NOTION_PROJET) et ajoute une colonne
    ID_BO_clean (entier) basÃ©e sur 'ID Back-office' ou 'ðŸ”‘ ID Back-office'.
    """
    try:
        schema_order = get_database_schema(DB_NOTION_PROJET)
        results = get_all_rows(DB_NOTION_PROJET)
    except Exception as e:
        st.warning(f"Impossible de charger la base Notion projets : {e}")
        return pd.DataFrame()

    if not results:
        return pd.DataFrame()

    df = notion_to_df(results, schema_order)

    # Cherche le bon nom de colonne pour l'ID BO
    id_bo_col = None
    for col in df.columns:
        if col in ("ID Back-office", "ðŸ”‘ ID Back-office"):
            id_bo_col = col
            break

    if id_bo_col is not None:
        df["ID_BO_clean"] = (
            pd.to_numeric(df[id_bo_col], errors="coerce")
              .astype("Int64")  # entier nullable
        )
    else:
        # Colonne absente : on met juste une colonne vide
        df["ID_BO_clean"] = pd.Series([pd.NA] * len(df), dtype="Int64")

    return df

from datetime import datetime

def get_vote_result_date_for_project(project_id: int) -> Optional[str]:
    df = load_notion_projects_df()
    if df is None or df.empty:
        return None

    try:
        pid = int(project_id)
    except Exception:
        return None

    match = df[df["ID_BO_clean"] == pid]
    if match.empty:
        return None

    row = match.iloc[0]
    val = row.get("Date rÃ©sultat de vote")

    if val is None:
        return None
    if isinstance(val, float) and math.isnan(val):
        return None

    raw = str(val).strip()

    # cas "start â†’ end" (plage de dates)
    if "â†’" in raw:
        start_raw, end_raw = [x.strip() for x in raw.split("â†’", 1)]
        def fmt(d):
            try:
                return datetime.fromisoformat(d.split("T")[0]).strftime("%d/%m/%Y")
            except Exception:
                return d
        return f"{fmt(start_raw)} â†’ {fmt(end_raw)}"

    # cas simple : une seule date
    try:
        d = datetime.fromisoformat(raw.split("T")[0])
        return d.strftime("%d/%m/%Y")
    except Exception:
        return raw
######################################################################################

def get_database_schema(db_id: str):
    """RÃ©cupÃ¨re la liste (nom, type) de toutes les propriÃ©tÃ©s de la base, dans l'ordre API."""
    url = f"https://api.notion.com/v1/databases/{db_id}"
    r = requests.get(url, headers=HEADERS)
    if r.status_code != 200:
        raise Exception(f"Erreur schÃ©ma Notion : {r.text}")
    data = r.json()
    props = data.get("properties", {})
    return [(name, definition.get("type")) for name, definition in props.items()]


def get_all_rows(db_id: str):
    """RÃ©cupÃ¨re toutes les pages d'une base Notion (pagination)."""
    url = f"https://api.notion.com/v1/databases/{db_id}/query"
    payload = {"page_size": 100}
    results = []
    while True:
        r = requests.post(url, headers=HEADERS, json=payload)
        if r.status_code != 200:
            raise Exception(f"Erreur Notion : {r.text}")
        data = r.json()
        results.extend(data.get("results", []))
        if not data.get("has_more"):
            break
        payload["start_cursor"] = data.get("next_cursor")
    return results


def _join_plain_text(rich_list):
    if not rich_list:
        return None
    try:
        return "".join([r.get("plain_text", "") for r in rich_list]) or None
    except Exception:
        return None


def parse_property_value(prop: dict):
    """
    Convertit un property Notion -> valeur simple.
    On couvre au max les types, le reste en JSON pour ne rien perdre.
    """
    if not isinstance(prop, dict):
        return prop
    t = prop.get("type")
    value = prop.get(t)

    # texte
    if t in ("title", "rich_text"):
        return _join_plain_text(value)

    # select / status
    if t in ("select", "status"):
        return value["name"] if value else None

    # multi_select
    if t == "multi_select":
        if not value:
            return None
        return ", ".join(v.get("name", "") for v in value)

    # number
    if t == "number":
        return value

    # date
    if t == "date":
        if not value:
            return None
        start = value.get("start")
        end = value.get("end")
        return f"{start} â†’ {end}" if end else start

    # checkbox
    if t == "checkbox":
        return value

    # url / email / phone
    if t in ("url", "email", "phone_number"):
        return value

    # people
    if t == "people":
        if not value:
            return None
        names = [
            p.get("name") or p.get("id") for p in value if p.get("name") or p.get("id")
        ]
        return ", ".join(names) if names else None

    # files
    if t == "files":
        if not value:
            return None
        parts = []
        for f in value:
            name = f.get("name")
            f_type = f.get("type")
            url = None
            if f_type == "file":
                url = f.get("file", {}).get("url")
            elif f_type == "external":
                url = f.get("external", {}).get("url")
            if name and url:
                parts.append(f"{name} ({url})")
            elif url:
                parts.append(url)
            elif name:
                parts.append(name)
        return ", ".join(parts) if parts else None

    # formula
    if t == "formula":
        if not value:
            return None
        f_type = value.get("type")
        if f_type in ("string", "number", "boolean"):
            return value.get(f_type)
        if f_type == "date":
            d = value.get("date")
            if not d:
                return None
            s = d.get("start")
            e = d.get("end")
            return f"{s} â†’ {e}" if e else s
        return json.dumps(value, ensure_ascii=False)

    # relation
    if t == "relation":
        if not value:
            return None
        return ", ".join(rel.get("id", "") for rel in value) or None

    # rollup
    if t == "rollup":
        if not value:
            return None
        r_type = value.get("type")
        if r_type in ("number", "boolean"):
            return value.get(r_type)
        if r_type == "date":
            d = value.get("date")
            if not d:
                return None
            s = d.get("start")
            e = d.get("end")
            return f"{s} â†’ {e}" if e else s
        if r_type == "array":
            items_txt = []
            for it in value.get("array", []):
                it_type = it.get("type")
                it_val = it.get(it_type)
                if isinstance(it_val, list):
                    txt = _join_plain_text(it_val)
                    if txt:
                        items_txt.append(txt)
                elif isinstance(it_val, dict):
                    if "name" in it_val:
                        items_txt.append(it_val["name"])
                else:
                    if it_val is not None:
                        items_txt.append(str(it_val))
            return ", ".join(items_txt) if items_txt else None
        return json.dumps(value, ensure_ascii=False)

    # created_time / last_edited_time
    if t in ("created_time", "last_edited_time"):
        return value

    # created_by / last_edited_by
    if t in ("created_by", "last_edited_by"):
        if not value:
            return None
        return value.get("name") or value.get("id")

    # unique_id
    if t == "unique_id":
        if not value:
            return None
        prefix = value.get("prefix") or ""
        number = value.get("number")
        if prefix and number is not None:
            return f"{prefix}-{number}"
        if number is not None:
            return str(number)
        return json.dumps(value, ensure_ascii=False)

    # fallback : JSON
    return json.dumps({k: v for k, v in prop.items() if k != "id"}, ensure_ascii=False)


def notion_to_df(results, schema_order):
    """Construit un DataFrame API Notion avec une colonne par propriÃ©tÃ©."""
    prop_names = [name for name, _ in schema_order]
    rows = []
    for page in results:
        props = page.get("properties", {})
        row = {}
        for name in prop_names:
            prop_value = props.get(name)
            row[name] = (
                parse_property_value(prop_value) if prop_value is not None else None
            )
        rows.append(row)
    return pd.DataFrame(rows, columns=prop_names)


# =============================================================================
# ðŸ”§ FONCTIONS IMPORT ZIP / CSV (Data Hub)
# =============================================================================

def extract_csv_recursive(zip_file):
    """
    Ouvre un ZIP Notion (ZIP dans ZIP possible) et renvoie une liste
    de (nom, DataFrame) pour tous les CSV trouvÃ©s.
    """
    csv_files = []

    def explore(zip_bytes):
        z = zipfile.ZipFile(zip_bytes)
        for name in z.namelist():
            if name.lower().endswith(".csv"):
                with z.open(name) as f:
                    csv_files.append((name, pd.read_csv(f)))
            elif name.lower().endswith(".zip"):
                nested_bytes = BytesIO(z.read(name))
                explore(nested_bytes)

    explore(zip_file)
    return csv_files


# =============================================================================
# ðŸ”§ FONCTIONS POSTGRESQL (Data Hub) â€” psycopg2
# =============================================================================

@st.cache_resource(show_spinner=False)
def get_pg_connection():
    conn = psycopg2.connect(
        host=PGHOST,
        port=PGPORT,
        dbname=PGDATABASE,
        user=PGUSER,
        password=PGPASSWORD,
        sslmode=PGSSLMODE,
    )
    return conn


@st.cache_data(show_spinner=False)
def list_pg_tables():
    """Renvoie un DataFrame (table_schema, table_name) pour toutes les tables utilisateur."""
    conn = get_pg_connection()
    query = """
    SELECT table_schema, table_name
    FROM information_schema.tables
    WHERE table_type = 'BASE TABLE'
      AND table_schema NOT IN ('pg_catalog', 'information_schema')
    ORDER BY table_schema, table_name;
    """
    return pd.read_sql(query, conn)


@st.cache_data(show_spinner=False)
def read_pg_table(schema: str, table: str) -> pd.DataFrame:
    """Lit la table PostgreSQL sans planter sur les dates invalides (date/timestamp => texte)."""
    conn = get_pg_connection()

    # 1) rÃ©cupÃ©rer les colonnes + types
    meta_q = """
    SELECT column_name, data_type
    FROM information_schema.columns
    WHERE table_schema = %s
      AND table_name = %s
    ORDER BY ordinal_position;
    """
    meta = pd.read_sql(meta_q, conn, params=(schema, table))

    # 2) construire un SELECT "safe"
    safe_cols = []
    for _, r in meta.iterrows():
        col = r["column_name"]
        typ = str(r["data_type"]).lower()
        if typ in ("date", "timestamp without time zone", "timestamp with time zone"):
            safe_cols.append(f'"{col}"::text AS "{col}"')
        else:
            safe_cols.append(f'"{col}"')

    query = f'SELECT {", ".join(safe_cols)} FROM "{schema}"."{table}"'
    return pd.read_sql(query, conn)


# =============================================================================
# ðŸ“„ MODULE 1 : PAGE "Votes Airtable â†” Souscriptions BO"
# =============================================================================

def page_votes():
    st.title("ðŸ“Š VÃ©rification des votes Airtable")

    # ----- Sidebar spÃ©cifique Votes -----
    with st.sidebar:
        st.header("âš™ï¸ Votes Airtable â†” BO")
        st.caption(
            "Lien de vÃ©rification Airtable : "
            "https://airtable.com/appjOQoptI7Av1obe/tblpoKvFoobl4yej0/viwbPANJvZO7AVX3A?blocks=hide"
        )
        if st.button("ðŸ”„ Purger caches Votes"):
            load_projects_df.clear()
            load_subs_for_project.clear()
            load_invest_amounts_for_project.clear()
            st.success("Caches Votes vidÃ©s.")
            st.experimental_rerun()

    with st.spinner("Chargement rÃ©fÃ©rentiel projets BOâ€¦"):
        projects_df = load_projects_df()

    # 1) Lister les vues Airtable contenant 'vote' ET dont la TABLE a prolongation ou pouvoir
    if not AIRTABLE_BASE_ID or not AIR_H:
        st.error(
            "Configure AIRTABLE_TOKEN / AIRTABLE_BASE_ID dans le fichier pour interroger Airtable."
        )
        st.stop()

    tables = list_tables_with_views(AIRTABLE_BASE_ID)
    vote_re = re.compile(r"\bvote[s]?\b", re.I)
    views = []
    for t in tables:
        if not table_has_prolongation_or_pouvoir(t):
            continue
        for v in t.get("views", []) or []:
            if vote_re.search(v.get("name", "")):
                views.append((t["name"], t["id"], v["name"], v["id"]))
    views = sorted(views, key=lambda x: (x[0].lower(), x[2].lower()))

    st.subheader("1) SÃ©lection de la vue Airtable (tableau de votes)")

    if not views:
        st.warning(
            "Aucune vue Ã©ligible trouvÃ©e (nom contenant 'vote' ET table avec colonne "
            "'prolongation' ou 'pouvoir')."
        )
        st.stop()

    labels = [f"{tname} â€¢ {vname}" for (tname, tid, vname, vid) in views]

    q = st.text_input(
        "Rechercher par mot-clÃ© ou coller lâ€™URL dâ€™une vue Airtable (optionnel)",
        placeholder="ex: Solaire ou https://airtable.com/app.../tbl.../viw...",
    ).strip()

    selected_by_url = None
    if q.startswith("http"):
        m = re.search(r"airtable\.com/([A-Za-z0-9]+)/([A-Za-z0-9]+)/([A-Za-z0-9]+)", q)
        if m:
            base_from_url, tbl_from_url, view_from_url = m.groups()
            if base_from_url == AIRTABLE_BASE_ID:
                selected_by_url = (tbl_from_url, view_from_url)
            else:
                st.info(
                    "âš ï¸ Lâ€™URL ne correspond pas Ã  la base configurÃ©e dans ce script "
                    "(AIRTABLE_BASE_ID)."
                )

    # --- SÃ©lection de la vue ---
    if selected_by_url:
        tid, vid = selected_by_url
        tname = next((t for (t, _tid, _v, _vid) in views if _tid == tid), f"Table {tid}")
        vname = next((v for (_t, _tid, v, _vid) in views if _vid == vid), f"Vue {vid}")
    else:
        if q and not q.startswith("http"):
            filt_idx = [i for i, lab in enumerate(labels) if q.lower() in lab.lower()]
            if not filt_idx:
                st.info("Aucun rÃ©sultat pour ce filtre. Affichage de toutes les vues Ã©ligibles.")
                options = labels
                idx_map = list(range(len(labels)))
            else:
                options = [labels[i] for i in filt_idx]
                idx_map = filt_idx
        else:
            options = labels
            idx_map = list(range(len(labels)))

        choice = st.selectbox(
            "Choisis un tableau de votes (vue Airtable) :",
            options,
            index=0,
        )
        pick = idx_map[options.index(choice)]
        tname, tid, vname, vid = views[pick]

    air_url = f"https://airtable.com/{AIRTABLE_BASE_ID}/{tid}/{vid}"
    st.markdown(f"ðŸ”— **Lien Airtable :** [{tname} â€¢ {vname}]({air_url})")

    # --- NOUVEAU : bouton pour charger la vue sÃ©lectionnÃ©e ---
    load_votes = st.button("âš™ï¸ Charger les donnÃ©es Airtable")

    if not load_votes:
        # On s'arrÃªte ici tant que l'utilisateur n'a pas cliquÃ©
        st.info("SÃ©lectionne une vue puis clique sur **âš™ï¸ Charger les donnÃ©es Airtable**.")
        st.stop()

    # Ã€ partir d'ici : tout le reste de ta logique inchangÃ©e
    with st.spinner("RÃ©cupÃ©ration de la vue Airtableâ€¦"):
        df_view = flatten(fetch_view_records(AIRTABLE_BASE_ID, tid, vid))

    st.write(f"**Vue :** {tname} â€¢ {vname} â€” {len(df_view):,} lignes")
    st.dataframe(df_view, use_container_width=True)

    # 2) RÃ©solution du projet via BO (name + department)
    st.subheader("2) RÃ©solution du projet (rÃ©fÃ©rence BO name + department)")
    project_id, project_disp = pick_project_id_from_airtable(
        df_view,
        projects_df,
        vname,
        tname,
    )
    proj_url = LPB_PROJECT_URL.format(project_id=project_id)
    st.markdown(f"ðŸ”— **Projet choisi :** {project_disp} â†’ [{proj_url}]({proj_url})")
    # ðŸ”Ž Date rÃ©sultat de vote depuis Notion (via ID Back-office)
    date_res_vote = get_vote_result_date_for_project(project_id)
    if date_res_vote:
        st.info(f"ðŸ“… **Date rÃ©sultat de vote (Notion)** : {date_res_vote}")
    else:
        st.info("ðŸ“… **Date rÃ©sultat de vote (Notion)** : non renseignÃ©e pour ce projet.")

    # 3) Emails & rÃ©ponses Airtable (prolongation + pouvoirs)
    st.subheader("3) Emails & RÃ©ponses Airtable (prolongation / pÃ©nalitÃ©s)")

    prolong_col = detect_prolongation_column(df_view)
    pouvoir_col = detect_pouvoir_column(df_view)

    if prolong_col is None and pouvoir_col is None:
        st.error(
            "âŒ Cette vue ne contient ni colonne liÃ©e Ã  la prolongation ni colonne liÃ©e aux pouvoirs.\n\n"
            "Merci de sÃ©lectionner une autre vue ou dâ€™ajouter ces colonnes dans la vue Airtable."
        )
        st.stop()

    email_cols = detect_email_columns(df_view)
    if not email_cols:
        st.error("Impossible de dÃ©tecter une colonne e-mail dans la vue (aucun '@').")
        st.stop()

    picked_em_cols = st.multiselect(
        "Colonnes e-mail Ã  utiliser :",
        options=email_cols,
        default=email_cols[:1],
    )

    st.caption(
        f"Colonne 'prolongation' dÃ©tectÃ©e : **{prolong_col or 'Aucune'}** â€” "
        f"colonne 'pouvoir' dÃ©tectÃ©e : **{pouvoir_col or 'Aucune'}**"
    )

    if pouvoir_col is not None:
        st.markdown(
            "**Questions associÃ©es :** \n"
            "- **Prolongation** : _Â« ÃŠtes-vous d'accord pour accorder la prolongation ? Â»_ \n"
            "- **Pouvoir (pÃ©nalitÃ©s)** : _Â« ÃŠtes-vous d'accord pour **NE PAS APPLIQUER** les pÃ©nalitÃ©s ? Â»_"
        )
    else:
        st.markdown(
            "**Question associÃ©e :** \n"
            "- **Prolongation** : _Â« ÃŠtes-vous d'accord pour appliquer la prolongation, "
            "avec application des 5% de pÃ©nalitÃ© ? Â»_"
        )

    votes_clean = build_votes_email_flags(df_view, picked_em_cols, prolong_col, pouvoir_col)

    # Doublons
    dups = votes_clean[votes_clean["n_occur"] > 1].sort_values("n_occur", ascending=False)
    st.write("Adresses en doublon (aprÃ¨s normalisation) :")
    if dups.empty:
        st.success("Aucun doublon dÃ©tectÃ© âœ…")
    else:
        st.dataframe(
            dups.rename(
                columns={"email_raw_example": "Adresse complÃ¨te", "n_occur": "Nombre de doublons"}
            ),
            use_container_width=True,
        )
        st.download_button(
            "ðŸ’¾ Exporter les doublons (CSV)",
            data=(
                dups[["email_normalized", "email_raw_example", "n_occur"]]
                .rename(
                    columns={
                        "email_raw_example": "adresse_complete",
                        "n_occur": "nombre_doublons",
                    }
                )
                .to_csv(index=False)
                .encode("utf-8")
            ),
            file_name="adresses_doublons.csv",
            mime="text/csv",
        )

    st.write(
        f"Emails uniques dans la vue (aprÃ¨s normalisation/dÃ©duplication) : **{len(votes_clean):,}**"
    )

    # 4) Souscriptions du projet (BO)
    st.subheader("4) Souscriptions BO du projet (filtrÃ©es par project_id)")
    subs = load_subs_for_project(project_id)

    # ðŸ”¢ Calcul du nombre d'investisseurs uniques (emails uniques)
    total_investors = (
        subs["email_normalized"]
        .dropna()
        .astype(str)
        .str.strip()
        .str.lower()
        .nunique()
    )
    
    st.write(f"Souscriptions uniques (avec e-mail) : {total_investors:,}")

    # 5) Croisement e-mail (Airtable â†” BO)
    st.subheader("5) Croisement e-mail (Airtable â†” BO)")
    merged = votes_clean.merge(
        subs[["email_normalized", "users_profile_id", "subscription_id", "subscribed_at"]],
        on="email_normalized",
        how="left",
    )

    # Garder la souscription la plus rÃ©cente si plusieurs
    merged = (
        merged.sort_values(["email_normalized", "subscribed_at"], ascending=[True, False])
        .drop_duplicates("email_normalized")
    )

    # A. Votants sans souscription
    off_proj = merged[merged["subscription_id"].isna()].copy().sort_values("email_normalized")
    st.write("Adresses sans souscription dÃ©tectÃ©e :")
    if off_proj.empty:
        st.success("Toutes les adresses ont au moins une souscription dÃ©tectÃ©e âœ…")
    else:
        st.dataframe(
            off_proj[["email_raw_example", "email_normalized", "prolongation", "pouvoir"]]
            .rename(columns={"email_raw_example": "Adresse complÃ¨te"}),
            use_container_width=True,
        )
        st.download_button(
            "ðŸ’¾ Exporter les adresses sans souscription (CSV)",
            data=(
                off_proj[
                    ["email_raw_example", "email_normalized", "prolongation", "pouvoir"]
                ]
                .rename(columns={"email_raw_example": "adresse_complete"})
                .to_csv(index=False)
                .encode("utf-8")
            ),
            file_name=f"adresses_sans_souscription_project_{project_id}.csv",
            mime="text/csv",
        )

    st.markdown("---")
    st.subheader("ðŸ“Š KPI rÃ©sultats")

    # Table finale = emails avec souscription
    final_tbl = merged[merged["subscription_id"].notna()].copy()
    final_tbl = final_tbl[
        [
            "email_raw_example",
            "email_normalized",
            "users_profile_id",
            "subscription_id",
            "subscribed_at",
            "prolongation",
            "pouvoir",
        ]
    ].rename(columns={"email_raw_example": "Adresse complÃ¨te"})

    # ===================== POIDS (â‚¬ investis) â€” INFORMATIONS =====================
    weights_df = load_invest_amounts_for_project(project_id)
    
    final_tbl = final_tbl.merge(weights_df, on="email_normalized", how="left")
    final_tbl["invest_amount_eur"] = final_tbl["invest_amount_eur"].fillna(0.0)


    n_votes = len(votes_clean)
    if not dups.empty:
        n_dups_total = int(dups["n_occur"].sum())
        n_dups_unique = int(len(dups))
        n_dups = int((dups["n_occur"] - 1).sum())
    else:
        n_dups_total = n_dups_unique = n_dups = 0

    n_with = final_tbl.shape[0]
    n_without = off_proj.shape[0]
    coverage_rate = (n_with / n_votes * 100) if n_votes else 0.0
    total_subs = (
        subs["email_normalized"].dropna().astype(str).str.strip().str.lower().nunique()
    )
    participation_rate = (n_with / total_subs * 100) if total_subs else 0.0
    part_delta = f"{n_with}/{total_subs}" if total_subs else "0/0 (aucune souscription projet)"
    # On rÃ©utilise / sÃ©curise la date de rÃ©sultat de vote Notion
    date_res_vote = date_res_vote if "date_res_vote" in locals() else get_vote_result_date_for_project(project_id)


    c1, c2, c3, c4, c5, c6 = st.columns(6)
    c1.metric("Adresses uniques (vue)", f"{n_votes:,}")
    c2.metric("Doublons", f"{n_dups:,}")
    c3.metric("Avec souscription", f"{n_with:,}")
    c4.metric("Sans souscription", f"{n_without:,}")
    c5.metric("Taux de couverture", f"{coverage_rate:.1f}%")
    c6.metric("Participation rÃ©elle", f"{participation_rate:.1f}%", delta=part_delta)
    
    c7, _, _ = st.columns(3)
    with c7:
        st.metric("Date rÃ©sultat de vote (Notion)", date_res_vote or "Non renseignÃ©e")


    st.caption(
        "KPI calculÃ©s aprÃ¨s dÃ©duplication. "
        "â€˜Couvertureâ€™ = adresses uniques de la vue avec souscription. "
        "â€˜Participationâ€™ = souscripteurs ayant rÃ©pondu / souscripteurs totaux du projet."
    )

    # =========================== RÃ©partition Prolongation & Pouvoirs ===========================
    st.subheader("â“ RÃ©partition des rÃ©ponses par question (votes lÃ©gitimes)")

    if n_with == 0:
        st.warning(
            "Aucune souscription BO n'a pu Ãªtre associÃ©e aux adresses de la vue. "
            "Il n'y a donc **aucun vote lÃ©gitime** pour ce projet : "
            "les rÃ©partitions de rÃ©ponses et le verdict ne sont pas calculÃ©s."
        )

        st.subheader("ðŸ§¾ Verdict final")
        st.info(
            "Impossible de calculer un verdict : aucun vote lÃ©gitime "
            "(0 investisseur avec souscription associÃ©e Ã  cette vue)."
        )

        st.markdown(
            "**Table finale â€” aprÃ¨s nettoyage (aucune ligne car aucune souscription associÃ©e au projet)**"
        )
        st.dataframe(final_tbl.sort_values("email_normalized"), use_container_width=True)
        st.download_button(
            "ðŸ’¾ Export CSV (table finale)",
            data=final_tbl.to_csv(index=False).encode("utf-8"),
            file_name=f"votes_x_subs_project_{project_id}_final.csv",
            mime="text/csv",
        )
        st.stop()

    base_df = final_tbl.copy()
    acteur_label = "les investisseurs (votes lÃ©gitimes, e-mails avec souscription)"

    st.caption(
        "Les rÃ©partitions et le verdict ci-dessous sont calculÃ©s **uniquement** "
        "sur les investisseurs identifiÃ©s (votes lÃ©gitimes, associÃ©s Ã  une souscription LPB)."
    )

    def render_pie(counts, title: str):
        """Retourne une figure matplotlib pour un camembert propre."""
        fig, ax = plt.subplots()
        ax.pie(
            counts["Nombre"],
            labels=counts.index,
            autopct="%1.1f%%",
            startangle=90,
        )
        ax.set_title(title)
        ax.axis("equal")
        return fig

    # ---- Analyse Prolongation ----
    counts_pro = None
    df_pro = None
    q_pro = ""

    if prolong_col is not None and "prolongation" in base_df.columns:
        s_pro = base_df["prolongation"].fillna("Non renseignÃ©").astype(str).str.strip()
        counts_pro = s_pro.value_counts()
        df_pro = counts_pro.reset_index()
        df_pro.columns = ["RÃ©ponse", "Nombre"]
        df_pro["%"] = (df_pro["Nombre"] / df_pro["Nombre"].sum() * 100).round(1)
        if pouvoir_col is not None:
            q_pro = "ÃŠtes-vous d'accord pour ACCORDER la prolongation ?"
        else:
            q_pro = (
                "ÃŠtes-vous d'accord pour APPLIQUER la prolongation, "
                "avec application des 5% de pÃ©nalitÃ©s ?"
            )

    # ---- Analyse Pouvoir (PÃ©nalitÃ©s) ----
    counts_pvr = None
    df_pvr = None
    q_pvr = ""

    if pouvoir_col is not None and "pouvoir" in base_df.columns:
        s_pvr = base_df["pouvoir"].fillna("Non renseignÃ©").astype(str).str.strip()
        counts_pvr = s_pvr.value_counts()
        df_pvr = counts_pvr.reset_index()
        df_pvr.columns = ["RÃ©ponse", "Nombre"]
        df_pvr["%"] = (df_pvr["Nombre"] / df_pvr["Nombre"].sum() * 100).round(1)
        q_pvr = "ÃŠtes-vous d'accord pour NE PAS APPLIQUER les pÃ©nalitÃ©s ?"

    col1, col2 = st.columns(2)
    if counts_pro is not None:
        with col1:
            st.markdown("### Question â€” Prolongation")
            st.caption(q_pro)
            st.dataframe(df_pro, use_container_width=True)
            fig_pro = render_pie(df_pro.set_index("RÃ©ponse"), "Prolongation")
            st.pyplot(fig_pro, clear_figure=True)

    if counts_pvr is not None:
        with col2:
            st.markdown("### Question â€” PÃ©nalitÃ©s")
            st.caption(q_pvr)
            st.dataframe(df_pvr, use_container_width=True)
            fig_pvr = render_pie(df_pvr.set_index("RÃ©ponse"), "PÃ©nalitÃ©s")
            st.pyplot(fig_pvr, clear_figure=True)

    # ===================== RÃ‰SULTAT PRORATISÃ‰ (INFORMATIF) =====================
    st.markdown("---")
    st.subheader("âš–ï¸ RÃ©sultat proratisÃ© (informatif) â€” pondÃ©rÃ© par le montant investi (â‚¬)")
    
    def weighted_counts(df: pd.DataFrame, col_answer: str, col_weight: str = "invest_amount_eur"):
        s = df[col_answer].fillna("Non renseignÃ©").astype(str).str.strip()
        tmp = pd.DataFrame({"RÃ©ponse": s, "Poids (â‚¬)": df[col_weight].fillna(0.0)})
        out = tmp.groupby("RÃ©ponse", as_index=False)["Poids (â‚¬)"].sum()
        total = float(out["Poids (â‚¬)"].sum())
        out["%"] = ((out["Poids (â‚¬)"] / total) * 100).round(1) if total > 0 else 0.0
        out = out.sort_values("Poids (â‚¬)", ascending=False)
        return out, total
    
    def render_pie_weighted(df_counts: pd.DataFrame, title: str):
        fig, ax = plt.subplots()
        ax.pie(
            df_counts["Poids (â‚¬)"],
            labels=df_counts["RÃ©ponse"],
            autopct="%1.1f%%",
            startangle=90,
        )
        ax.set_title(title)
        ax.axis("equal")
        return fig
    
    col1w, col2w = st.columns(2)
    
    # Prolongation pondÃ©rÃ©e
    if counts_pro is not None and "prolongation" in final_tbl.columns:
        with col1w:
            st.markdown("### Prolongation â€” pondÃ©rÃ©")
            df_w_pro, tot_w_pro = weighted_counts(final_tbl, "prolongation")
            st.dataframe(df_w_pro, use_container_width=True)
            st.caption(f"Total pondÃ©rÃ© : {tot_w_pro:,.2f} â‚¬")
            st.pyplot(render_pie_weighted(df_w_pro, "Prolongation (pondÃ©rÃ©)"), clear_figure=True)
    
    # PÃ©nalitÃ©s / pouvoir pondÃ©rÃ©
    if counts_pvr is not None and "pouvoir" in final_tbl.columns:
        with col2w:
            st.markdown("### PÃ©nalitÃ©s â€” pondÃ©rÃ©")
            df_w_pvr, tot_w_pvr = weighted_counts(final_tbl, "pouvoir")
            st.dataframe(df_w_pvr, use_container_width=True)
            st.caption(f"Total pondÃ©rÃ© : {tot_w_pvr:,.2f} â‚¬")
            st.pyplot(render_pie_weighted(df_w_pvr, "PÃ©nalitÃ©s (pondÃ©rÃ©)"), clear_figure=True)
    

    # ================================ Verdict final ================================
    st.subheader("ðŸ§¾ Verdict final")

    def get_yes_no(counts):
        """Retourne uniquement les Oui/Non sous forme (yes, no, total_exprimes)."""
        yes = int(counts.get("Oui", 0))
        no = int(counts.get("Non", 0))
        total_exprimes = yes + no
        return yes, no, total_exprimes

    verdict_parts = []

    # Verdict prolongation
    if counts_pro is not None:
        yes_p, no_p, tot_p = get_yes_no(counts_pro)
        if tot_p == 0:
            verdict_parts.append(
                "Sur la question de la prolongation, aucun vote Oui/Non exploitable n'a Ã©tÃ© exprimÃ©."
            )
        else:
            if pouvoir_col is not None:
                # Cas avec pÃ©nalitÃ©s sÃ©parÃ©es
                if yes_p > no_p:
                    verdict_parts.append(
                        f"Sur la question de la prolongation, {acteur_label} "
                        f"**ACCEPTENT la prolongation** "
                        f"({yes_p} Oui / {tot_p} votes exprimÃ©s)."
                    )
                elif no_p > yes_p:
                    verdict_parts.append(
                        f"Sur la question de la prolongation, {acteur_label} "
                        f"**REFUSENT la prolongation** "
                        f"({no_p} Non / {tot_p} votes exprimÃ©s)."
                    )
                else:
                    verdict_parts.append(
                        "Sur la question de la prolongation, il y a **Ã©galitÃ© parfaite** "
                        f"({yes_p} Oui / {no_p} Non). DÃ©cision manuelle nÃ©cessaire."
                    )
            else:
                # Cas sans colonne pouvoir : prolongation + pÃ©nalitÃ©s
                if yes_p > no_p:
                    verdict_parts.append(
                        "Sur la question Â« prolongation avec 5% de pÃ©nalitÃ©s Â», "
                        f"{acteur_label} **ACCEPTENT la prolongation avec pÃ©nalitÃ©s** "
                        f"({yes_p} Oui / {tot_p} votes exprimÃ©s)."
                    )
                elif no_p > yes_p:
                    verdict_parts.append(
                        "Sur la question Â« prolongation avec 5% de pÃ©nalitÃ©s Â», "
                        f"{acteur_label} **REFUSENT la prolongation avec pÃ©nalitÃ©s** "
                        f"({no_p} Non / {tot_p} votes exprimÃ©s)."
                    )
                else:
                    verdict_parts.append(
                        "Sur la question Â« prolongation avec 5% de pÃ©nalitÃ©s Â», il y a "
                        f"**Ã©galitÃ© parfaite** ({yes_p} Oui / {no_p} Non). "
                        "DÃ©cision manuelle nÃ©cessaire."
                    )

    # Verdict pÃ©nalitÃ©s (pouvoir)
    if counts_pvr is not None:
        yes_pen, no_pen, tot_pen = get_yes_no(counts_pvr)
        if tot_pen == 0:
            verdict_parts.append(
                "Sur la question des pÃ©nalitÃ©s, aucun vote Oui/Non exploitable n'a Ã©tÃ© exprimÃ©."
            )
        else:
            if yes_pen > no_pen:
                verdict_parts.append(
                    "Sur la question des pÃ©nalitÃ©s, "
                    f"{acteur_label} **VALIDENT la non-application des pÃ©nalitÃ©s** "
                    f"({yes_pen} Oui / {tot_pen} votes exprimÃ©s)."
                )
            elif no_pen > yes_pen:
                verdict_parts.append(
                    "Sur la question des pÃ©nalitÃ©s, "
                    f"{acteur_label} **REFUSENT la non-application des pÃ©nalitÃ©s** "
                    f"({no_pen} Non / {tot_pen} votes exprimÃ©s)."
                )
            else:
                verdict_parts.append(
                    "Sur la question des pÃ©nalitÃ©s, il y a **Ã©galitÃ© parfaite** "
                    f"({yes_pen} Oui / {no_pen} Non). DÃ©cision manuelle nÃ©cessaire."
                )

    if not verdict_parts:
        st.markdown("Impossible de calculer un verdict : aucune donnÃ©e exploitable.")
    else:
        for v in verdict_parts:
            st.markdown("âž¡ï¸ " + v)

    # ================================ Table finale + export ================================
    st.markdown(
        "**Table finale â€” aprÃ¨s nettoyage (doublons supprimÃ©s) et retrait des invests sans souscription**"
    )
    st.dataframe(final_tbl.sort_values("email_normalized"), use_container_width=True)
    st.download_button(
        "ðŸ’¾ Export CSV (table finale)",
        data=final_tbl.to_csv(index=False).encode("utf-8"),
        file_name=f"votes_x_subs_project_{project_id}_final.csv",
        mime="text/csv",
    )


# =============================================================================
# ðŸ“„ MODULE 2 : PAGE "Data Hub LPB (Notion + Back-office)"
# =============================================================================

def page_data_hub():
    st.title("ðŸ› ï¸ Data Hub (BO/Notion)")
    st.caption("Connexion Notion & Back-office en lecture (read-replica).")

    # ---- Sidebar spÃ©cifique Data Hub ----
    with st.sidebar:
        section = st.radio(
            "Data Hub â€” Section",
            ["Notion", "Back-office"], #  ["Notion", "Back-office"] -> BO annulÃ© finalement
            index=0,
        )

    # ============================ NOTION ============================
    if section == "Notion":
        st.subheader("ðŸ  Notion")
        st.caption(
            "Connexion API Notion ou import de l'export Notion (ZIP/CSV) via : "
            "https://www.notion.so/lapremierebrique/20a18ece1f2d81fab4bbf17f57df8a3a?v=21418ece1f2d8031baf2000c82651d2f"
        )

        # ðŸ‘‰ Drag & drop (ZIP/CSV) en premier, API en deuxiÃ¨me
        tab_import, tab_api = st.tabs(["Mode Export (ZIP/CSV)", "Mode API (plus long)"])

        # ---------- Mode Import ZIP/CSV ----------
        with tab_import:
            st.subheader("Import dâ€™un export Notion (ZIP/CSV)")
            uploaded = st.file_uploader(
                "DÃ©pose ici ton export Notion : soit le ZIP principal, soit un CSV",
                type=["zip", "csv"],
                accept_multiple_files=False,
            )

            if uploaded is not None:
                try:
                    if uploaded.name.lower().endswith(".csv"):
                        df_import = pd.read_csv(uploaded)
                        chosen_name = uploaded.name
                    else:
                        with st.spinner("Lecture du ZIP Notion (ZIP â†’ ZIP â†’ CSV)â€¦"):
                            csv_list = extract_csv_recursive(uploaded)
                        if not csv_list:
                            st.error("Aucun CSV trouvÃ© dans ce ZIP.")
                            df_import = None
                            chosen_name = None
                        else:
                            all_candidates = [
                                c for c in csv_list if c[0].lower().endswith("_all.csv")
                            ]
                            if len(all_candidates) == 1:
                                chosen_name, df_import = all_candidates[0]
                            else:
                                chosen_name = st.selectbox(
                                    "Plusieurs CSV trouvÃ©s, choisis celui Ã  afficher :",
                                    [name for name, _ in csv_list],
                                )
                                df_import = next(
                                    df for (name, df) in csv_list if name == chosen_name
                                )

                    if df_import is not None:
                        st.session_state.imp_df = df_import
                        st.session_state.imp_filename = chosen_name
                        st.session_state.pop("imp_cols_multiselect", None)
                except Exception as e:
                    st.error(f"âŒ Erreur lors de la lecture de lâ€™export : {e}")

            df_import = st.session_state.imp_df
            chosen_name = st.session_state.imp_filename

            if df_import is None:
                st.info("Glisse ton fichier ici pour commencer.")
            else:
                st.success(
                    f"Fichier chargÃ© : **{chosen_name}** "
                    f"({len(df_import)} lignes Ã— {len(df_import.columns)} colonnes)"
                )

                all_cols_imp = list(df_import.columns)
                if "imp_cols_multiselect" not in st.session_state:
                    st.session_state.imp_cols_multiselect = all_cols_imp.copy()
                else:
                    st.session_state.imp_cols_multiselect = [
                        c
                        for c in st.session_state.imp_cols_multiselect
                        if c in all_cols_imp
                    ]

                st.caption("Colonnes Ã  afficher (tu peux taper pour filtrer les noms).")
                c2_sel1, c2_sel2 = st.columns(2)
                with c2_sel1:
                    if st.button("âœ… Tout sÃ©lectionner", key="imp_select_all"):
                        st.session_state.imp_cols_multiselect = all_cols_imp.copy()
                with c2_sel2:
                    if st.button("ðŸš« Tout dÃ©sÃ©lectionner", key="imp_select_none"):
                        st.session_state.imp_cols_multiselect = []

                st.multiselect(
                    "Colonnes Ã  afficher",
                    options=all_cols_imp,
                    key="imp_cols_multiselect",
                )

                selected_cols_imp = st.session_state.imp_cols_multiselect
                if selected_cols_imp:
                    df_view_imp = df_import[selected_cols_imp].copy()
                else:
                    df_view_imp = df_import.iloc[:, []].copy()

                edited_df_imp = st.data_editor(
                    df_view_imp,
                    use_container_width=True,
                    height=550,
                    hide_index=True,
                    key="imp_table",
                )

                # âœ… limiter export (global)
                df_export_imp = edited_df_imp.head(MAX_ROWS_EXPORT).copy()
                
                # CSV : si gros => ZIP en plusieurs CSV
                csv_is_chunked_imp = len(df_export_imp) > ROWS_PER_CSV_FILE
                if csv_is_chunked_imp:
                    zip_name_imp = f"{TODAY_STR}_notion_export_csv.zip"
                    zip_bytes_imp = chunk_df_to_zip_csv_bytes(
                        df_export_imp,
                        rows_per_file=ROWS_PER_CSV_FILE,
                        base_name=f"{TODAY_STR}_notion_export",
                    )
                else:
                    csv_name_imp = f"{TODAY_STR}_notion_export.csv"
                    csv_bytes_imp = df_to_csv_bytes(df_export_imp)
                
                # Excel : limite stricte
                df_excel_imp = df_export_imp.head(MAX_ROWS_EXCEL).copy()
                xlsx_name_imp = f"{TODAY_STR}_notion_export.xlsx"
                excel_bytes_imp = df_to_excel_bytes(df_excel_imp, sheet_name="ExportNotion")
                
                c1, c2 = st.columns(2)
                with c1:
                    if csv_is_chunked_imp:
                        st.download_button("ðŸ“¥ TÃ©lÃ©charger CSV", data=zip_bytes_imp, file_name=zip_name_imp, mime="application/zip")
                    else:
                        st.download_button("ðŸ“¥ TÃ©lÃ©charger CSV", data=csv_bytes_imp, file_name=csv_name_imp, mime="text/csv")
                
                with c2:
                    st.download_button("ðŸ“¥ TÃ©lÃ©charger Excel", data=excel_bytes_imp, file_name=xlsx_name_imp,
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                
                if len(edited_df_imp) > MAX_ROWS_EXPORT:
                    st.warning(f"âš ï¸ Export tronquÃ© : {MAX_ROWS_EXPORT:,} premiÃ¨res lignes sur {len(edited_df_imp):,}.")


        # ---------- Mode API ----------
        with tab_api:
            st.subheader("Connexion directe Ã  Notion (API)")
            base_choisie = st.radio(
                "SÃ©lection",
                ["Notion projet", "Bac Ã  sable"],
                horizontal=True,
            )
            db_id = DB_BAC_SABLE if base_choisie == "Bac Ã  sable" else DB_NOTION_PROJET

            if st.button("âš™ï¸ Charger via lâ€™API Notion"):
                try:
                    with st.spinner("RÃ©cupÃ©ration du schÃ©ma Notionâ€¦"):
                        schema_order = get_database_schema(db_id)
                    with st.spinner("RÃ©cupÃ©ration de toutes les lignesâ€¦"):
                        results = get_all_rows(db_id)
                    if not results:
                        st.info("Aucune page trouvÃ©e dans cette base.")
                        st.session_state.api_df = None
                    else:
                        df_api = notion_to_df(results, schema_order)
                        st.session_state.api_df = df_api
                        st.session_state.pop("api_cols_multiselect", None)
                except Exception as e:
                    st.error(f"âŒ Erreur API Notion : {e}")

            df_api = st.session_state.api_df
            if df_api is None or df_api.empty:
                st.info(
                    "Aucune donnÃ©e API chargÃ©e pour le moment. "
                    "Clique sur le bouton ci-dessus."
                )
            else:
                st.success(f"{len(df_api)} lignes â€¢ {len(df_api.columns)} colonnes")

                all_cols = list(df_api.columns)
                if "api_cols_multiselect" not in st.session_state:
                    st.session_state.api_cols_multiselect = all_cols.copy()
                else:
                    st.session_state.api_cols_multiselect = [
                        c
                        for c in st.session_state.api_cols_multiselect
                        if c in all_cols
                    ]

                st.caption("Colonnes Ã  afficher (tu peux taper pour filtrer les noms).")
                c_sel1, c_sel2 = st.columns(2)
                with c_sel1:
                    if st.button("âœ… Tout sÃ©lectionner", key="api_select_all"):
                        st.session_state.api_cols_multiselect = all_cols.copy()
                with c_sel2:
                    if st.button("ðŸš« Tout dÃ©sÃ©lectionner", key="api_select_none"):
                        st.session_state.api_cols_multiselect = []

                st.multiselect(
                    "Colonnes Ã  afficher",
                    options=all_cols,
                    key="api_cols_multiselect",
                )

                selected_cols = st.session_state.api_cols_multiselect
                if selected_cols:
                    df_view = df_api[selected_cols].copy()
                else:
                    df_view = df_api.iloc[:, []].copy()

                edited_df = st.data_editor(
                    df_view,
                    use_container_width=True,
                    height=550,
                    hide_index=True,
                    key="api_table",
                )

                base_slug = base_choisie.replace(" ", "_").lower()

                # âœ… limiter export (global)
                df_export_api = edited_df.head(MAX_ROWS_EXPORT).copy()
                
                # CSV : si gros => ZIP en plusieurs CSV
                csv_is_chunked_api = len(df_export_api) > ROWS_PER_CSV_FILE
                if csv_is_chunked_api:
                    zip_name = f"{TODAY_STR}_notion_api_{base_slug}_csv.zip"
                    zip_bytes = chunk_df_to_zip_csv_bytes(
                        df_export_api,
                        rows_per_file=ROWS_PER_CSV_FILE,
                        base_name=f"{TODAY_STR}_notion_api_{base_slug}",
                    )
                else:
                    csv_name = f"{TODAY_STR}_notion_api_{base_slug}.csv"
                    csv_bytes = df_to_csv_bytes(df_export_api)
                
                # Excel : limite stricte
                df_excel_api = df_export_api.head(MAX_ROWS_EXCEL).copy()
                xlsx_name = f"{TODAY_STR}_notion_api_{base_slug}.xlsx"
                excel_bytes = df_to_excel_bytes(df_excel_api, sheet_name=base_choisie[:31])
                
                c1, c2 = st.columns(2)
                with c1:
                    if csv_is_chunked_api:
                        st.download_button("ðŸ“¥ TÃ©lÃ©charger CSV", data=zip_bytes, file_name=zip_name, mime="application/zip")
                    else:
                        st.download_button("ðŸ“¥ TÃ©lÃ©charger CSV", data=csv_bytes, file_name=csv_name, mime="text/csv")
                
                with c2:
                    st.download_button("ðŸ“¥ TÃ©lÃ©charger Excel", data=excel_bytes, file_name=xlsx_name,
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                
                if len(edited_df) > MAX_ROWS_EXPORT:
                    st.warning(f"âš ï¸ Export tronquÃ© : {MAX_ROWS_EXPORT:,} premiÃ¨res lignes sur {len(edited_df):,}.")


    # ============================ BACK-OFFICE ============================
    elif section == "Back-office":
        st.subheader("ðŸ—„ Back-office PostgreSQL")
        st.caption("Connexion en lecture au read-replica PostgreSQL de production.")

        with st.expander("â„¹ï¸ Aide â€” Exports & limites", expanded=False):
            st.markdown(
                """
        **Comment fonctionnent les exports :**
        
        - Les exports sont **limitÃ©s automatiquement** pour Ã©viter les crashs (RAM Streamlit limitÃ©e).
        - **CSV** :
          - petit volume â†’ **1 CSV**
          - gros volume â†’ **ZIP avec plusieurs CSV** (pagination automatique)
        - **Excel** :
          - export **limitÃ©** (fichiers Excel trop gros = plantage)
        
        Si un export est tronquÃ©, un message sâ€™affiche.
        
        ---
        
        **Fichiers CSV paginÃ©s (ZIP)**  
        Quand un ZIP est gÃ©nÃ©rÃ©, il contient plusieurs fichiers :
        `export_001.csv`, `export_002.csv`, etc.  
        âž¡ï¸ Ils peuvent Ãªtre **fusionnÃ©s facilement** en local (Python / Excel PowerQuery).
        
        ---
        
        âš ï¸ **Tables Ã  Ã©viter absolument (trop volumineuses)**  
        Ces tables peuvent faire planter Streamlit mÃªme en lecture :
        - `subscriptions`
        - `payment_operations`
        - `loans_terms`
        
        ðŸ‘‰ Si besoin, utiliser **des filtres forts** (project_id, dates, statut)  
        ou passer par un outil SQL dÃ©diÃ©.
                """
            )        

        # Connexion & liste des tables
        try:
            with st.spinner("Connexion Ã  PostgreSQL et rÃ©cupÃ©ration des tablesâ€¦"):
                tables_df = list_pg_tables()
                st.session_state.bo_tables_df = tables_df
            st.success("âœ… Connexion PostgreSQL rÃ©ussie")
        except Exception as e:
            st.error(f"âŒ Erreur de connexion PostgreSQL : {e}")
            st.stop()

        tables_df = st.session_state.bo_tables_df
        if tables_df is None or tables_df.empty:
            st.info("Aucune table utilisateur trouvÃ©e dans cette base.")
            st.stop()

        # Optionnel : schÃ©ma image si tu as le fichier
        try:
            st.image("assets/schema_bo.png")
        except Exception:
            pass

        tables_df["full_name"] = tables_df["table_schema"] + "." + tables_df["table_name"]

        def make_label(row):
            if row["table_schema"] == "public":
                return row["table_name"]
            else:
                return row["table_schema"] + "." + row["table_name"]

        tables_df["label"] = tables_df.apply(make_label, axis=1)
        table_labels = tables_df["label"].tolist()
        full_names = tables_df["full_name"].tolist()
        label_to_fullname = dict(zip(table_labels, full_names))

        default_full = st.session_state.bo_table_id or full_names[0]
        if default_full in full_names:
            default_label = tables_df.loc[
                tables_df["full_name"] == default_full, "label"
            ].iloc[0]
        else:
            default_label = table_labels[0]

        selected_label = st.selectbox(
            "Table PostgreSQL",
            options=table_labels,
            index=table_labels.index(default_label),
            help="Tu peux taper ici pour filtrer les tables en temps rÃ©el.",
        )

        selected_full = label_to_fullname[selected_label]
        st.session_state.bo_table_id = selected_full
        schema, table = selected_full.split(".", 1)

        # --- boutons cÃ´te Ã  cÃ´te (Charger / RafraÃ®chir) ---
        b_load, b_refresh = st.columns([2, 1])
        
        with b_load:
            load_clicked = st.button("âš™ï¸ Charger via Postgre", use_container_width=True)
        
        with b_refresh:
            if st.button("ðŸ”„ RafraÃ®chir", use_container_width=True):
                st.rerun()  # Ã©quivalent Ctrl+R
        
        if load_clicked:
            try:
                with st.spinner(f"Lecture de {schema}.{table}"):
                    df_bo = read_pg_table(schema, table)
                st.session_state.bo_df = df_bo
                st.session_state.pop("bo_cols_multiselect", None)
            except Exception as e:
                st.error(f"âŒ Erreur lors de la lecture de la table : {e}")


        df_bo = st.session_state.bo_df
        if df_bo is None or df_bo.empty:
            st.info(
                "Aucune donnÃ©e chargÃ©e pour le moment. "
                "Choisis une table puis clique sur le bouton."
            )
        else:
            st.success(f"{len(df_bo)} lignes chargÃ©es â€¢ {len(df_bo.columns)} colonnes")

            all_cols_bo = list(df_bo.columns)
            if "bo_cols_multiselect" not in st.session_state:
                st.session_state.bo_cols_multiselect = all_cols_bo.copy()
            else:
                st.session_state.bo_cols_multiselect = [
                    c
                    for c in st.session_state.bo_cols_multiselect
                    if c in all_cols_bo
                ]

            st.caption("Colonnes Ã  afficher (tu peux taper pour filtrer les noms).")
            b1, b2 = st.columns(2)
            with b1:
                if st.button("âœ… Tout sÃ©lectionner", key="bo_select_all"):
                    st.session_state.bo_cols_multiselect = all_cols_bo.copy()
            with b2:
                if st.button("ðŸš« Tout dÃ©sÃ©lectionner", key="bo_select_none"):
                    st.session_state.bo_cols_multiselect = []

            st.multiselect(
                "Colonnes Ã  afficher",
                options=all_cols_bo,
                key="bo_cols_multiselect",
            )

            selected_cols_bo = st.session_state.bo_cols_multiselect
            if selected_cols_bo:
                df_bo_view = df_bo[selected_cols_bo].copy()
            else:
                df_bo_view = df_bo.iloc[:, []].copy()

            edited_bo = st.data_editor(
                df_bo_view,
                use_container_width=True,
                height=550,
                hide_index=True,
                key="bo_table",
            )

            safe_table_name = selected_label.replace(".", "_")

            # âœ… limiter export (global)
            df_export_bo = edited_bo.head(MAX_ROWS_EXPORT).copy()
            
            # CSV : si gros => ZIP en plusieurs CSV
            csv_is_chunked_bo = len(df_export_bo) > ROWS_PER_CSV_FILE
            if csv_is_chunked_bo:
                zip_name_bo = f"{TODAY_STR}_{safe_table_name}_csv.zip"
                zip_bytes_bo = chunk_df_to_zip_csv_bytes(
                    df_export_bo,
                    rows_per_file=ROWS_PER_CSV_FILE,
                    base_name=f"{TODAY_STR}_{safe_table_name}",
                )
            else:
                csv_name_bo = f"{TODAY_STR}_{safe_table_name}.csv"
                csv_bytes_bo = df_to_csv_bytes(df_export_bo)
            
            # Excel : limite stricte
            df_excel_bo = df_export_bo.head(MAX_ROWS_EXCEL).copy()
            xlsx_name_bo = f"{TODAY_STR}_{safe_table_name}.xlsx"
            excel_bytes_bo = df_to_excel_bytes(df_excel_bo, sheet_name=safe_table_name[:31])
            
            c1, c2 = st.columns(2)
            with c1:
                if csv_is_chunked_bo:
                    st.download_button("ðŸ“¥ TÃ©lÃ©charger CSV", data=zip_bytes_bo, file_name=zip_name_bo, mime="application/zip")
                else:
                    st.download_button("ðŸ“¥ TÃ©lÃ©charger CSV", data=csv_bytes_bo, file_name=csv_name_bo, mime="text/csv")
            
            with c2:
                st.download_button("ðŸ“¥ TÃ©lÃ©charger Excel", data=excel_bytes_bo, file_name=xlsx_name_bo,
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            
            if len(edited_bo) > MAX_ROWS_EXPORT:
                st.warning(f"âš ï¸ Export tronquÃ© : {MAX_ROWS_EXPORT:,} premiÃ¨res lignes sur {len(edited_bo):,}.")

# =============================================================================
# ðŸ“„ MODULE 3/4 : Autres pages
# =============================================================================

def page_prequalification():
    st.title("ðŸ“ PrÃ©qualification des projets, emailing de refus et scoring ML de la prÃ©diction du statut")
    st.markdown(
        """
        Cette app publique est dÃ©diÃ©e Ã  l'Ã©quipe projet.

        ðŸ‘‰ **Ouvrir l'application Streamlit :**  
        [https://lpb-pdp-preselection-emaling.streamlit.app/](https://lpb-pdp-preselection-emaling.streamlit.app/)
        """
    )

def page_rendements():
    st.title("ðŸ“ˆ Comparaison des rendements par classe d'actifs")

    st.markdown(
        """
        Dashboard interactif comparant le rendement du crowdfunding immobilier (â‰ˆ 11,5 % annualisÃ©)
        aux autres classes d'actifs avec une Ã©tude des corrÃ©lations.

        ðŸ‘‰ **Ouvrir l'application Rshiny :**  
        [https://rtaugourdeau-lpb.shinyapps.io/RendementActifs/](https://rtaugourdeau-lpb.shinyapps.io/RendementActifs/)
        """
    )

def page_docs():
    st.title("ðŸ“„ Documentation")

    DOCS = {
        "ðŸ“Š Listing des dashboards PBI": "https://docs.google.com/document/d/129lJRvAsvrxj4P2L_bN5zfhMeF5aXm0YV5wSJ6yLOT8/edit?tab=t.0",
        "ðŸ—„ï¸ Architecture Fabric & MODOP Power BI": "https://docs.google.com/document/d/1MJ7R0YZRqa54vfT9ID8l5upATsqZ39I-82hATI4rCaQ/edit?tab=t.0",
        "ðŸ”„ Synchronisation ETL & Alerting emailing": "https://docs.google.com/document/d/1YU93c_Gi0MvFghxcAAd7YXAnriuLxTK_C3lYrJmnhnY/edit?tab=t.0",
    }
    for label, url in DOCS.items():
        st.markdown(f"**{label}** : [{url}]({url})")

# =============================================================================
# ðŸ§­ ROUTAGE PRINCIPAL
# =============================================================================

def main():
    with st.sidebar:
        st.markdown("## ðŸ§± Outils Data LPB")

        app_choice = st.radio(
            "Choix de lâ€™outil",
            [
                "Data Hub (BO/Notion)",
                "VÃ©rification des votes Airtable",
                "PrÃ©qual + Emailing + Scoring ML",
                "Comparaison des rendements actifs",
                "Documentation",
            ],
            index=0,
        )

    if app_choice == "VÃ©rification des votes Airtable":
        page_votes()
    elif app_choice == "PrÃ©qual + Emailing + Scoring ML":
        page_prequalification()
    elif app_choice == "Comparaison des rendements actifs":
        page_rendements()
    elif app_choice == "Documentation":
        page_docs()
    else:
        page_data_hub()

if __name__ == "__main__":
    main()






















































